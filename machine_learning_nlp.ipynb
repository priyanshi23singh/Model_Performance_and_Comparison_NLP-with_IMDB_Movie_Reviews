{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn and Natural Language Processing\n",
    "In this part, we will apply sklearn and related NLP libraries to perform data analysis on the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading a subset of the dataset, which contains 5000 movie reviews and their associated sentiment labels (i.e., whether a review is considered positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"imdb_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taran Adarsh a reputed critic praised such a d...</td>\n",
       "      <td>taran adarsh repute critic praise dubba movie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth entertainment value rental especially li...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I liked Antz, but loved \"A Bug's Life\". The an...</td>\n",
       "      <td>like antz love bug life animation put paid def...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This reboot is like a processed McDonald's mea...</td>\n",
       "      <td>reboot like process mcdonald meal compare ang ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The working title was: \"Don't Spank Baby\". &lt;br...</td>\n",
       "      <td>work title spank baby wayne crawford go become...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Taran Adarsh a reputed critic praised such a d...   \n",
       "1  Worth the entertainment value of a rental, esp...   \n",
       "2  I liked Antz, but loved \"A Bug's Life\". The an...   \n",
       "3  This reboot is like a processed McDonald's mea...   \n",
       "4  The working title was: \"Don't Spank Baby\". <br...   \n",
       "\n",
       "                                    processed_review sentiment  \n",
       "0  taran adarsh repute critic praise dubba movie ...  negative  \n",
       "1  worth entertainment value rental especially li...  negative  \n",
       "2  like antz love bug life animation put paid def...  positive  \n",
       "3  reboot like process mcdonald meal compare ang ...  negative  \n",
       "4  work title spank baby wayne crawford go become...  positive  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review` column contains raw review texts from the original dataset. However, it's always a good idea to process and clean text data before performing analysis. The column `processed_review` was constructed by processing and tokenizing the raw reviews, and then joining the review tokens by a single space. From this point, we only need to focus on the `processed_review` and `sentiment` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the distribution of class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    2500\n",
       "positive    2500\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will be ignored by the autograder\n",
    "display(df_reviews['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 2500 positive reviews and 2500 negative reviews. In other words, our dataset is [perfectly balanced](https://i.imgflip.com/303krn.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Count Vectorizer\n",
    "\n",
    "The first feature construction task we will perform is building a term-frequency matrix. The function `count_vectorizer` uses sklearn's `CountVectorizer` API to construct the term-frequency training matrix and testing matrix, along with the feature names (i.e., the list of words corresponding to the columns in the matrices).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(x):\n",
    "    return x\n",
    "\n",
    "def count_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the term-frequency matrices for train_data and test_data using CountVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "        \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF matrices\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer=str.split, tokenizer=str.split, preprocessor=dummy_fun)\n",
    "    tf_train = vectorizer.fit_transform(reviews_train)\n",
    "    if reviews_test is not None:\n",
    "        tf_test = vectorizer.transform(reviews_test)\n",
    "    else:\n",
    "        tf_test = None\n",
    "    features = vectorizer.get_feature_names_out().tolist()\n",
    "    \n",
    "    return tf_train, tf_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_count_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    count_vec_train, count_vec_test, features = count_vectorizer(reviews_train, reviews_test)\n",
    "    assert count_vec_train.shape == (3750, 27242)\n",
    "    assert count_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        count_vec_train.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [70, 65, 168, 77, 139, 132, 28, 139, 453, 89]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        count_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [168, 60, 59, 144, 494, 135, 69, 119, 76, 68]\n",
    "    )\n",
    "    assert features[:10] == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert features[-10:] == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_count_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF Vectorizer\n",
    "\n",
    "Now let's use the TF-IDF feature construction method. The function `tfidf_vectorizer`  uses sklearn's `TfidfVectorizer` API to construct the TF-IDF training matrix and testing matrices, along with the feature names (i.e., the list of words corresponding to the columns in the matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(reviews_train, reviews_test = None):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF matrices for train_data and test_data using TfidfVectorizer.\n",
    "    \n",
    "    args:\n",
    "        reviews_train (pd.Series[str]) : a Series of processed reviews for training\n",
    "    \n",
    "    kwargs:\n",
    "        reviews_test (pd.Series[str]) : a Series of processed reviews for testing\n",
    "    \n",
    "    return:\n",
    "        Tuple(tf_train, tf_test, features):\n",
    "            tf_train (scipy.sparse.csr_matrix) : TF-IDF matrix for training\n",
    "            tf_test (scipy.sparse.csr_matrix) : TF-IDF matrix for testing,\n",
    "                or None if reviews_test is None\n",
    "            features (List[str]) : the list of words corresponding to the columns in the TF-IDF matrices\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    tf_train = vectorizer.fit_transform(reviews_train)\n",
    "    \n",
    "    if reviews_test is not None:\n",
    "        tf_test = vectorizer.transform(reviews_test)\n",
    "    else:\n",
    "        tf_test = None\n",
    "    \n",
    "    features = vectorizer.get_feature_names_out().tolist()\n",
    "    \n",
    "    return tf_train, tf_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_tfidf_vectorizer():\n",
    "    reviews_train, reviews_test = train_test_split(df_reviews[\"processed_review\"], random_state = 0)\n",
    "    tfidf_vec_trains, tfidf_vec_test, features = tfidf_vectorizer(reviews_train, reviews_test)\n",
    "    assert tfidf_vec_trains.shape == (3750, 27242)\n",
    "    assert tfidf_vec_test.shape == (1250, 27242)\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_trains.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.03658925089979, 7.417196035144321, 11.492434722367015, 6.965673648338525, 9.428219597939362, 9.425632229448961, 3.9722806270035345, 9.635230284023372, 11.779155501275017, 7.44670396016231]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        tfidf_vec_test.sum(axis = 1)[:10].ravel().tolist()[0],\n",
    "        [7.2233277330801196, 4.869804242110142, 6.249091468966529, 9.689812079503804, 11.89432945296538, 9.115185225757216, 6.798492438570971, 8.57464867777901, 7.954528809138947, 6.81383392701789]\n",
    "    )\n",
    "    assert features[:10] == ['00', '000', '00015', '007', '00pm', '00s', '01', '01pm', '02', '029']\n",
    "    assert features[-10:] == ['zucco', 'zucker', 'zukovic', 'zula', 'zuleika', 'zumhofe', 'zurer', 'zvezda', 'zwick', 'zylberstein']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predicting review sentiment\n",
    "Let's now see which feature construction method -- TF or TF-IDF -- is better for predicting review sentiments in our dataset. Our choice of learning algorithm here will be a support vector machine with Gaussian kernel (this means that it uses a different hypothesis function that can also account for non-linearly separable data).\n",
    "\n",
    "The function `predict_sentiment` takes as input the `reviews` and `sentiment` columns of our IMDB dataset and performs the following tasks:\n",
    "1. Convert the `sentiment` column to a vector `y` of 1s and -1s: `positive` corresponds to 1 and `negative` to -1.\n",
    "1. Perform a [stratified k-fold split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) of the review and sentiment vectors, based on the provided `k`. Also set `shuffle` to `True` and `random_state` to the provided `seed`.\n",
    "1. For $f$ from $1 \\to k$:\n",
    "     * Let fold $f$ be the test set, and the remaining $k-1$ folds be the training set.\n",
    "     * Convert the training and testing reviews to feature matrices `X_train` and `X_test`, using either TF or TF-IDF. Which method to use is based on the function parameter `method`.\n",
    "     * Train the SVM model on `X_train, y_train` and evaluate its accuracy $a_f$ on `X_test, y_test`.\n",
    "1. Return $a_1, a_2, \\ldots, a_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review_sentiment(reviews, sentiments, method, k, seed = 0):\n",
    "    \"\"\"\n",
    "    Compute the cross-validated accuracy of SVM with either TF or TF-IDF features\n",
    "    in predicting review sentiment.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "        sentiments (pd.Series[str]) : a Series of movie review sentiments,\n",
    "            containing either \"positive\" or \"negative\"\n",
    "        method (str) : a string which is either \"TF\" or \"TF-IDF\",\n",
    "            specifying which feature construction method to use\n",
    "        k (int) : the number of folds in stratified k-fold split\n",
    "    \n",
    "    kwargs:\n",
    "        seed (int) : the random generator seed for kfold split\n",
    "        \n",
    "    return:\n",
    "        List[float] : a list of k accuracy values from evaluating a trained SVM model\n",
    "            on each of the k folds, using the remaining folds as training data\n",
    "    \"\"\"\n",
    "    y = sentiments.map({'positive': 1, 'negative': -1}).values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(reviews, sentiments):\n",
    "        X_train, X_test = reviews.iloc[train_index], reviews.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if method == 'TF':\n",
    "            X_train_transformed, X_test_transformed, _ = count_vectorizer(X_train, X_test)\n",
    "        elif method == 'TF-IDF':\n",
    "            X_train_transformed, X_test_transformed, _ = tfidf_vectorizer(X_train, X_test)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose either 'TF' or 'TF-IDF'.\")\n",
    "        \n",
    "        svm = SVC(kernel=\"rbf\", C=10)\n",
    "        svm.fit(X_train_transformed, y_train)\n",
    "        \n",
    "        accuracy = svm.score(X_test_transformed, y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "Cross-validated accuracy of SVM with TF matrices 0.8389999999999999\n",
      "Cross-validated accuracy of SVM with TF-IDF matrices 0.8617999999999999\n"
     ]
    }
   ],
   "source": [
    "def test_predict_review_sentiment():\n",
    "    # prediction based on TF\n",
    "    count_vec_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF\", 10)\n",
    "    assert count_vec_accs == [0.878, 0.836, 0.854, 0.824, 0.826, 0.824, 0.824, 0.85, 0.844, 0.83]\n",
    "    \n",
    "    # prediction based on TF-IDF\n",
    "    tf_idf_accs = predict_review_sentiment(df_reviews[\"processed_review\"], df_reviews[\"sentiment\"], \"TF-IDF\", 10)\n",
    "    assert tf_idf_accs == [0.88, 0.862, 0.85, 0.868, 0.854, 0.846, 0.864, 0.874, 0.874, 0.846]\n",
    "    print(\"All tests passed!\")\n",
    "    print(\"Cross-validated accuracy of SVM with TF matrices\", np.mean(count_vec_accs))\n",
    "    print(\"Cross-validated accuracy of SVM with TF-IDF matrices\", np.mean(tf_idf_accs))\n",
    "    \n",
    "test_predict_review_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using TF-IDF features yields better cross-validated accuracy than using TF features (when the learning algorithm is SVM with RBF kernel and $C = 10$), although the difference in this case is not large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling and word distribution\n",
    "Let's now try to understand the review texts a bit more. We can treat all the reviews as a corpus and perform Latent Dirichlet Allocation to extract the corpus topics. We can also see which words are most frequent in a given topic. The function `top_words_by_topic` takes as input the `processed_reviews` column in our IMDB dataset and performs the following tasks:\n",
    "\n",
    "1. Build a term-frequency matrix out of this column.\n",
    "1. Input this matrix to sklearn's `LatentDirichletAllocation`.\n",
    "1. In the resulting word-topic matrix, identify the most frequent `n_top_words` in each topic. Sort the most frequent words from lower to higher frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_by_topic(reviews, n_topics = 10, n_top_words = 20, seed = 0):\n",
    "    \"\"\"\n",
    "    Perform topic modeling on the movie review corpus and return the most frequent words in each topic.\n",
    "    \n",
    "    args:\n",
    "        reviews (pd.Series[str]) : a Series of all processed movie reviews\n",
    "    \n",
    "    kwargs:\n",
    "        n_topics (int) : the number of topics to model by LDA\n",
    "        n_top_words (int) : the number of most frequent words to identify in each topic\n",
    "        seed (int) : the random generator seed for LDA\n",
    "    \n",
    "    return:\n",
    "        List[List[str]] : a nested list of words, where each of the n_topics inner lists\n",
    "            contains the n_top_words most frequent words in a given topic\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer=str.split, tokenizer=str.split, preprocessor=dummy_fun)\n",
    "    tf_matrix = vectorizer.fit_transform(reviews)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, learning_method=\"online\", random_state=seed)\n",
    "    lda.fit(tf_matrix)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        sorted_indices = topic.argsort()\n",
    "        top_indices = sorted_indices[:-n_top_words-1:-1]\n",
    "        topic_top_words = [feature_names[i] for i in top_indices]\n",
    "        top_words.append(topic_top_words[::-1])\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/Users/priyanshisingh/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_top_words_by_topic():\n",
    "    corpus = pd.Series([\n",
    "        \"I like to eat broccoli and bananas\",\n",
    "        \"I ate a banana and spinach smoothie for breakfast\",\n",
    "        \"Chinchillas and kittens are cute\",\n",
    "        \"My sister adopted a kitten yesterday\",\n",
    "        \"Look at this cute hamster munching on a piece of broccoli\"\n",
    "    ])\n",
    "    top_words = top_words_by_topic(corpus, n_topics = 2, n_top_words = 5)\n",
    "    assert top_words == [['Look', 'broccoli', 'and', 'cute', 'a'], ['I', 'eat', 'like', 'to', 'and']]\n",
    "    \n",
    "    top_words = top_words_by_topic(df_reviews[\"processed_review\"], n_topics = 5, n_top_words = 5)\n",
    "    assert top_words == [\n",
    "        ['performance', 'play', 'version', 'jack', 'role'],\n",
    "        ['dancer', 'paris', 'dance', 'cartoon', 'hitchcock'],\n",
    "        ['make', 'like', 'one', 'film', 'movie'],\n",
    "        ['film', 'father', 'world', 'american', 'war'],\n",
    "        ['mad', 'sheriff', 'match', 'carmen', 'arthur']\n",
    "    ]\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_top_words_by_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word embedding and word similarity\n",
    "Finally, let's look at how we can train a word embedding model from our movie review corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_words(reviews, input_words, n_similar_words):\n",
    "    corpus = [review.split() for review in reviews]\n",
    "    model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, workers = 4, min_count = 1)\n",
    "    similar_words = []\n",
    "    for inp_word in input_words:\n",
    "        similar_words.append([w for w in sorted(model.wv.most_similar(inp_word, topn = n_similar_words), key = lambda x: x[1])])\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'see':\n",
      "[('tonight', 0.9114551544189453), ('believe', 0.912100076675415), ('watch', 0.918253481388092), ('remember', 0.9189435839653015), ('dumbest', 0.9263791441917419), ('rent', 0.9340086579322815), ('heard', 0.9407606720924377)]\n",
      "\n",
      "Words most similar to 'good':\n",
      "[('funny', 0.9429802298545837), ('fetch', 0.9457199573516846), ('nice', 0.9502775073051453), ('decent', 0.9511047005653381), ('awful', 0.952179491519928), ('terrible', 0.959258496761322), ('horrible', 0.9593119621276855)]\n",
      "\n",
      "Words most similar to 'bad':\n",
      "[('fetch', 0.8861246705055237), ('horrible', 0.8951228857040405), ('strangest', 0.8966148495674133), ('terrible', 0.8991929292678833), ('good', 0.9007490873336792), ('awful', 0.9033360481262207), ('stupidest', 0.9163578748703003)]\n",
      "\n",
      "Words most similar to 'watch':\n",
      "[('wait', 0.9476494193077087), ('remember', 0.9490033984184265), ('recommend', 0.9496187567710876), ('subspecies', 0.950914740562439), ('enjoy', 0.9528793692588806), ('tonight', 0.9564355611801147), ('rent', 0.9795867800712585)]\n",
      "\n",
      "Words most similar to 'check':\n",
      "[('twice', 0.9927701950073242), ('agree', 0.9932975172996521), ('anyway', 0.993416965007782), ('expectation', 0.9940482378005981), ('trailer', 0.9941006302833557), ('negative', 0.9946498274803162), ('avoid', 0.9967467188835144)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_find_most_similar_words():\n",
    "    input_words = [\"see\", \"good\", \"bad\", \"watch\", \"check\"]\n",
    "    most_similar_words = find_most_similar_words(df_reviews[\"processed_review\"], input_words, 7)\n",
    "    for i in range(len(input_words)):\n",
    "        print(f\"Words most similar to '{input_words[i]}':\")\n",
    "        print(most_similar_words[i])\n",
    "        print()\n",
    "    \n",
    "test_find_most_similar_words()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
